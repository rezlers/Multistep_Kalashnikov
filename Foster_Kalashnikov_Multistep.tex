
\documentclass[10pt, reqno]{amsart}
\usepackage[utf8]{inputenc}
\usepackage[T2A]{fontenc}
\usepackage[english,russian]{babel}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage[hidelinks,unicode]{hyperref}
\usepackage{indentfirst}
\usepackage{anyfontsize}
\usepackage{dsfont}
\usepackage{relsize}


\def\udcs{519.233} %Here the author places classificators of the paper according to Russian classification system
\def\mscs{62F03} %Here the author places  classificators according to the AMS classification list.
\setcounter{page}{1}

\newtheorem*{theorem*}{Theorem}
\newtheorem{repeated_theorem}{Theorem}
\newtheorem{numbered_theorem}{Утверждение}
\newtheorem{numbered_corollary}{Замечание}
\newtheorem{lemma}{Лемма}

\newcommand{\pee}{A} % path exists event
\newcommand{\aps}{\Pi} % all pathes set
\newcommand{\app}{\pi} % any particular path
\newcommand{\pps}{K} % pathes projections set
\newcommand{\lgc}{F} % linear growth coefficient
\newcommand{\cte}{c} % constant to estimate
\newcommand{\sas}{\mathcal{F}}

\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]

\newcommand{\aasVar}{Q} % any accessible set
\newcommand{\asaVar}{\mathfrak{B}} % any sigma-algebra
\newcommand{\astVar}{\tau} % any stopping time
\newcommand{\gtfVar}{G} % G theorem function
\newcommand{\wtfVar}{W} % W theorem function
\newcommand{\atoVar}{\mathbb{A}} % A theorem operator
\newcommand{\dooVar}{\mathbb{D}} % domain of operator (A)
\newcommand{\ltfVar}{L} % Lyapunov test function
\newcommand{\DeltaVar}{\Delta} % Lyapunov test function
\newcommand{\assVar}{\mathfrak{X}} % Lyapunov test function
\newcommand{\integers}{\mathbb{Z}} % Lyapunov test function
\newcommand{\reals}{\mathbb{R}} % Lyapunov test function
\newcommand{\akdVar}{\mathbf{D}_{\atoVar}} % A Kalashnikov's domain

\makeatletter
\newcommand*{\inlineequation}[3][]{%
	\begingroup
	% Put \refstepcounter at the beginning, because
	% package `hyperref' sets the anchor here.
	\refstepcounter{equation}%
	\ifx\\#1\\%
	\else
	\label{#1}%
	\tag{#2}
	\fi
	% prevent line breaks inside equation
	\relpenalty=10000 %
	\binoppenalty=10000 %
	\ensuremath{%
		% \displaystyle % larger fractions, ...
		#3%
	}%
	~\@eqnnum
	\endgroup
}
\makeatother

\usepackage{hyperref}

% \setmathfont{DejaVu Sans}
% DejaVu Sans: $\mathbb{\Gamma\gamma\Pi\pi\sum}$

\newcommand{\norm}[1]{\left\lVert#1\right\rVert}

\newcommand{\bbone}{\usefont{T2A}{bbold}{m}{n}\Pi}
\MakeRobust{\bbone}

\newcommand{\cgy}[6]{
	\text{П}\arraycolsep=1.4pt\def\arraystretch{0.87}
	\scriptsize
	\begin{array}{ccc}
		#2 & #4 & #6\\
		#1 & #3 & #5
	\end{array}
	\normalsize
}

\newcommand{\stcomp}[1]{\overline{#1}}

\DeclareUnicodeCharacter{2212}{-}
\DeclareMathOperator*{\E}{\mathbb{E}}
\DeclareMathOperator*{\Pb}{\mathbb{P}}

\def\logo{{\bf\huge S\raisebox{0.2ex}{\hspace{0.55ex}\raisebox{0.05ex}e\hspace{-1.65ex}$\bigcirc$}MR}}

\makeatletter
\def\@settitle{
	\begin{center}%
		\baselineskip14\p@\relax
		\bfseries
		\large
		\@title
	\end{center}%
}
\makeatother

\makeatletter
\newcommand{\leqnomode}{\tagsleft@true\let\veqno\@@leqno}
%\newcommand{\reqnomode}{\tagsleft@false\let\veqno\@@eqno}
\makeatother

% \def\semrtop
%      {
%   \vbox{
%      \noindent\logo
%      \hspace{80mm}\raisebox{1ex}{ISSN 1813-3304 }

%      \vspace{5mm}

%      \begin{center}
%      {\huge Journal} \\[2mm] 
%      {\large Journal} \\[1mm]
%      {\LARGE\tt{http://semr.math.nsc.ru}}\\[0.5mm]
% %     {\small 3 3 3 3 3}\\[-1mm]
% %     {\small Sobolev Institute of Mathematics SB RAS}
%      \end{center}
%      \vspace{-3mm}
%      \noindent
%      \begin{tabular}{c}
%      \hphantom{aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa} \\
%      \hline\hline
%      \end{tabular}

%      \vspace{1mm}
%      {\flushleft\it Том 16, стр. 144--144 (2019) \hspace{65mm}{\rm\small УДК \udcs}} %Volume, pages in Russian,
%                                                                                     %filled by Editorial board
%      \newline
%          {\rm\small DOI~10.33048/semi.2019.16.xxx}\hphantom{aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa}{\rm\small MSC\ \ \mscs }
%   }%\vbox
% }

\newcommand\myshorttitle{\small\textsc{Multistep generalisation of G-recurrency theorem}}
\begin{titlepage}
	\title[\myshorttitle]{Multistep generalisation of G-recurrency theorem}
	\author{{Chebunin M.G., Rezler A.V}}%
	
	\email{rezlers123@gmail.com}%
	\address{Mikhail Georgievich Chebunin
		\newline\hphantom{iii} Karlsruhe Institute of Technology,
		\newline\hphantom{iii} Institute of Stochastics,
		\newline\hphantom{iii} Karlsruhe, 76131, Germany.}%
	
	\email{chebuninmikhail@gmail.com}%
	
	\address{Alexandr Vadimovich Rezler
		\newline\hphantom{iii} Novosibirsk State University
		\newline\hphantom{iii} 2, Pirogova str.}%
	
	
	
	\thanks{\sc A. Rezler, M. Chebunin,
		Multistep generalisation of G-recurrency theorem}
	\thanks{\copyright \ 2023 Chebunin M.G., Rezler A.V}
\end{titlepage}
\begin{document}
	\renewcommand{\refname}{References}
	\renewcommand{\proofname}{Proof.}
	\renewcommand{\figurename}{Fig.}
	\thispagestyle{empty}
	
	
	% \semrtop \vspace{1cm}
	\maketitle {\small
		\begin{quote}
			\noindent{\sc Abstract.} 
			Among the various approaches of proving the ergodicity of a Markov chain, a fairly common method is based on the Foster criterion, during which it is necessary to construct a test function whose drift in one step of the chain satisfies certain properties. In the work of S.G. Foss and T. Konstantopoulos the generalization of the Foster criterion was proposed. It allows one to obtain the same results if the test function satisfies similar properties in some arbitrary number of steps. V.V. Kalashnikov, in his book “Mathematical methods in queuing theory”, studied Foster’s one-step criterion under more general conditions on the test function, which made it possible to obtain more general results. The goal of the paper is to generalise its results to the multi-step case.
			
			\medskip
			
			\noindent{\bf Keywords:} Markov chains, ergodicity, generalised Foster criterion, G-recurrency.
		\end{quote}
	}
	
	\section{V.V. Kalashnikov}
	Let $\aasVar \in \asaVar$, $\astVar_{\aasVar} = \min{n : n > 0, X_{n} \in \aasVar}$, and $\gtfVar(n) \geq 0$, $n \geq 0$, be a monotonic function such as $\lim_{n \xrightarrow{} \infty}\gtfVar(n) = \infty$.
	\begin{definition}
		A subset $\aasVar$ is called recurrent if for any $x \in \assVar$
		\begin{gather*}
			\Pb(\astVar_{\aasVar} < \infty \: | \: X_{0} = x) = 1.
		\end{gather*}
	\end{definition}
	\begin{definition}
		A subset $\aasVar \in \asaVar$ is called $\gtfVar$-recurrent if for any $x \in \assVar$
		\begin{gather}
			\E\{\gtfVar(\astVar_{\aasVar}) \: | \: X_{0} = x\} < \infty.
		\end{gather}
	\end{definition}
	If $\gtfVar(x) \equiv x$ then the subset $\aasVar$ is said to be (positive) recurrent.
	
	Recall the notation
	\begin{gather}
		\E{}_{x}\{\cdot\} := \E\{\cdot \: | \: X_{0} = x\}.
	\end{gather}
	
	\begin{theorem*}
		In order for the inequality $\E{}_{x}\gtfVar(\astVar_{\aasVar}) < \infty$ to hold for any $x \in \assVar$, it is necessary and sufficient that there exists a function $\wtfVar(x, n)$, $x \in \assVar$, $n \geq 0$, which satisfies the conditions:
		\begin{enumerate}
			\item[(a)] $\underset{x}{\inf}\wtfVar(x, n) \geq \gtfVar(n)$;
			\item[(b)] $\atoVar\wtfVar(x, n) := \int_{\assVar}\Pb(x, dx)\wtfVar(y, n+1) - \wtfVar(x, n) \leq 0$, $x \notin \aasVar$, $n \geq 0$;
			\item[(c)] $\atoVar\wtfVar(x, 0) < \infty$, $x \in \aasVar$.
		\end{enumerate}
		Under these conditions the following inequality is valid:
		\begin{gather}
			\E{}_{x}\gtfVar(\astVar_{\aasVar}) \leq \begin{cases}
				\wtfVar(x, 0), & x \notin \aasVar \\
				\wtfVar(x, 0) + \atoVar\wtfVar(x, 0), & x \in \aasVar
			\end{cases}
			\label{main_G_W_inequality}
		\end{gather}
	\end{theorem*}
	
	\begin{proof}
		Consider a new Markov chain $\{Y_{n}\}_{n \geq 0}$, where
		\begin{gather*}
			Y_{n} = (X_{n}, n), n \geq 0.
		\end{gather*}
		Thus, the state space of this "expanded" chain is $\assVar \times \integers_{+}$ and its transition function is
		\begin{gather*}
			\Pb(Y_{n+1} = (X_{n+1}, n+1), X_{n+1} \in B \: | \: X_{n}, n) = \Pb(X_{n} \: | \: B),
		\end{gather*}
		so the second component of $Y_{n}$ each time has an increment 1. Note that the r.v. $\astVar_{\aasVar}$ is a stopping time not only for $\{X_{n}\}$ but also for $\{Y_{n}\}$ either. The generating operator $\atoVar$ for $\{Y_{n}\}$ operates on functions defined on $\assVar \times \integers_{+}$ and the result of this is shown in the condition (b) of the Theorem.
		
		Let \wtfVar satisfy the conditions (a) - (c) of the Theorem and take the stopping time $\astVar_{n} = \min(n, \astVar_{\aasVar})$. Then, by Theorem 1.1 (Kalashnikov's book), we have
		\begin{gather}
			\E{}_{x}\gtfVar(\astVar_{n}) \leq \text{(by (a))} \leq \E{}_{(x, 0)}\wtfVar(X_{\astVar_{n}}, \astVar_{n}) \notag \\ = \wtfVar(x, 0) + \E{}_{(x, 0)}\sum_{k < \astVar_{n}}\atoVar\wtfVar(X_{k}, k).
			\label{expectation_Gtau_estimation}
		\end{gather}
		If $x \notin \aasVar$, then $X_{k} \notin \aasVar$ for all $k < \astVar_{n}$, and the condition (b) implies, that the second summand on the right-hand side of (\ref{expectation_Gtau_estimation}) is nonpositive. Thus,
		\begin{gather*}
			\E{}_{x}\gtfVar(\astVar_{n}) \leq \wtfVar(x, 0).
		\end{gather*}
		Letting $n \xrightarrow{} \infty$ we obtain the first line in (\ref{main_G_W_inequality}).
		
		If $x \in \aasVar$, then only the term $\atoVar\wtfVar(X_{0}, 0) = \atoVar\wtfVar(x, 0)$ (which is finite by (c)) on the right-hand side of (\ref{expectation_Gtau_estimation}) may be positive. It follows (letting again $n \xrightarrow{} \infty$) the second line in (\ref{main_G_W_inequality}).
		
		Now let us suppose that $\E{}_{x}\gtfVar(\astVar_{n}) < \infty$ for each $x \in \assVar$. Fix $n \geq 0$. Then, by the Markov property of the chain X, we have for any $x \in \assVar$
		\begin{gather}
			\E{}_{x}\gtfVar(n + \astVar_{\aasVar}) = \int_{\assVar\backslash\aasVar}\Pb(x, dz)\E{}_{x}\gtfVar(n + \astVar_{\aasVar} + 1) + \Pb(x, \aasVar)\gtfVar(n+1).
			\label{markov_property_for_expectation}
		\end{gather}
		Now let us take the function $\wtfVar : \assVar \times \integers_{+} \xrightarrow{} \reals^{1}$ of the form
		\begin{gather}
			\wtfVar(x, n) = \begin{cases}
				\E{}_{x}\gtfVar(n + \astVar_{\aasVar}), & x \notin Q,\\
				\gtfVar(n), & x \in \aasVar;
			\end{cases}
			\label{W_assumption}
		\end{gather}
		It is evident that (a) holds. The relation (\ref{markov_property_for_expectation}) can be rewritten for $x \notin \aasVar$ in the following way (using (\ref{W_assumption})):
		\begin{gather*}
			\wtfVar(x, n) = \int_{\assVar}\Pb(x, dz)\E{}_{z}\gtfVar(n + \astVar_{\aasVar} + 1),
		\end{gather*}
		which means that (b) is true for the function (\ref{W_assumption}). For $x \in \aasVar$ and $n = 0$, we have from (\ref{markov_property_for_expectation}). For $x \in \aasVar$ and $n = 0$, we have from (\ref{markov_property_for_expectation})
		\begin{gather*}
			\E{}_{x}\gtfVar(\astVar_{\aasVar}) = \int_{\assVar \backslash \aasVar}\Pb(x, dz)\E{}_{x}\gtfVar(1 + \astVar_{\aasVar}) + \Pb(x, \aasVar)\gtfVar(1) \\ \equiv \atoVar\wtfVar(x, 0) + \wtfVar(x, 0) = \atoVar\wtfVar(x, 0) + \gtfVar(0),
		\end{gather*}
		and so, $\atoVar\wtfVar(x, 0) = \E{}_{x}\gtfVar(\astVar_{\aasVar}) - \gtfVar(0) < \infty$, i.e. (c) holds too.
	\end{proof}
	
	\newpage
	
	\section{Generalised Foster's criterion}
	\newcommand{\hffVar}{h} % h theorem function
	\newcommand{\gffVar}{g} % h theorem function
	\newcommand{\cefVar}{\mathcal{E}} % h theorem function
	\newcommand{\indicator}{\mathds{1}} % h theorem function
	
	Let us assume that
	\begin{enumerate}
		\item[(L1)] $\hffVar$ is bounded below: $\inf_{x \in \assVar}\hffVar(x) > 0$;
		\item[(L2)] $\hffVar$ is eventually positive: $\liminf_{\ltfVar \xrightarrow{} \infty}\hffVar(x) > 0$;
		\item[(L3)] $\gffVar$ is locally bounded above: $\sup_{\ltfVar \leq N}\gffVar(x) < \infty$, for all $N > 0$;
		\item[(L4)] $\gffVar$ is eventually bounded by $\hffVar$: $\limsup\gffVar(x)/\hffVar(x) < \infty$.
	\end{enumerate}
	For a measurable set $\aasVar \subseteq \assVar$ define $\astVar_{\aasVar} = \inf\{n > 0 \: : \: X_{n} \in \aasVar\}$ to be the first return time to $\aasVar$. 
	\begin{definition}
		A subset $\aasVar$ is called recurrent if for any $x \in \aasVar$
		\begin{gather*}
			\Pb(\astVar_{\aasVar} < \infty \: | \: X_{0} = x) = 1.
		\end{gather*}
	\end{definition}
	\begin{definition}
		A subset $\aasVar \in \asaVar$ is called positive-recurrent if
		\begin{gather}
			\sup_{x \in \aasVar}\E\{\astVar_{\aasVar} \: | \: X_{0} = x\} < \infty.
		\end{gather}
	\end{definition}
	It is this last property that is determined by a suitably designed Lyapunov function. This is the content of Theorem 1 below. That this property can be translated into a stability statement is the subject of later sections.
	\begin{theorem*}
		Suppose that the drift of $\ltfVar$ in $\gffVar(x)$ step satisfies the "drift condition"
		\begin{gather*}
			\E{}_{x}\left[\ltfVar(X_{\gffVar(x)}) - \ltfVar(X_{0})\right] \leq -\hffVar(x),
		\end{gather*}
		where $\ltfVar, \gffVar, \hffVar$ satisfy (L1) - (L4). Let
		\begin{gather*}
			\astVar \equiv \astVar_{N} = \inf\{n > 0 \: :  \: \ltfVar(X_{n}) \leq N\}.
		\end{gather*}
		Then there exists $N_{0} > 0$, such that for all $N > N_{0}$ and any $x \in \assVar$, we have $\E{}_{x}\astVar < \infty$. Also, $\sup_{\ltfVar(x) \leq N}\E_{x}\astVar < \infty$.
	\end{theorem*}
	\begin{proof}
		From the drift condition, we obviously have that $\ltfVar(x) - \hffVar(x) \geq 0$ for all $x$. We choose $N_{0}$ such that $\inf_{\ltfVar(x) > N_{0}}\hffVar(x) > 0$ and $\sup_{\ltfVar(x) > N_{0}}\gffVar(x)/\hffVar(x) < \infty$. Then, for $N \geq N_{0}$, $\hffVar(x)$ strictly positive, and we set
		\begin{gather*}
			d = \sup_{\ltfVar(x) > N}\gffVar(x)/\hffVar(x).
		\end{gather*}
		Then $0 < d < \infty$ as follows from (L2) and (L4). We also let
		\begin{gather*}
			-H = \inf_{x \in \assVar}\hffVar(x)
		\end{gather*}
		and $H < \infty$, from (L1). We define an increasing sequence $t_{n}$ of stopping times recursively by
		\begin{gather*}
			t_{0} = 0, \: t_{n} = t_{n-1} + \gffVar(X_{t_{n-1}}), \: n \geq 1.
		\end{gather*}
		By the strong Markov property, the variables
		\begin{gather*}
			Y_{n} = X_{t_{n}}
		\end{gather*}
		form a Markov chain with, as easily proved by induction on $n$, $\E{}_{x}\ltfVar(Y_{n+1}) \leq \E{}_{x}\ltfVar(Y_{n}) + H$, and so $\E{}_{x}\ltfVar(Y_{n}) < \infty$ for all $n$ and $x$. Define the stopping time
		\begin{gather*}
			\gamma = \inf\{n \geq 1 \: : \: \ltfVar(Y_{n}) \leq N\} < \infty.
		\end{gather*}
		Observe that
		\begin{gather*}
			\astVar \leq t_{\gamma}, a.s.
		\end{gather*}
		Let $\asaVar_{n}$ be the sigma field generated by $Y_{0},..., Y_{n}$. We define the "cumulative energy"$ $ between 0 and $\gamma \wedge n$ by
		\begin{gather*}
			\cefVar_{n} = \sum_{i=0}^{\gamma \wedge n}\ltfVar(Y_{i}) = \sum_{i=0}^{n}\ltfVar(Y_{i})\indicator(\gamma \geq i)
		\end{gather*}
		and estimate the drift $\E{}_{x}(\cefVar_{n} - \cefVar_{0})$ (which is finite) in a "martingale fashion":
		\begin{gather*}
			\E{}_{x}(\cefVar_{n} - \cefVar_{0}) = \E{}_{x}\sum_{i=1}^{n}\E{}_{x}(\ltfVar(Y_{i})\indicator\left(\gamma \geq i) \: | \: \asaVar_{i-1}\right) \\= \E{}_{x}\sum_{i=1}^{n}\indicator(\gamma \geq i)\E{}_{x}\left(\ltfVar(Y_{i}) \: | \: \asaVar_{i-1}\right) \\ \leq \E{}_{x}\sum_{i=1}^{n}\indicator(\gamma \geq i)\E{}_{x}\left(\ltfVar(Y_{i-1}) - \hffVar(Y_{i-1}) \: | \: \asaVar_{i-1}\right) \\ \leq \E{}_{x}\sum_{i=1}^{n}\indicator(\gamma \geq i)\E{}_{x}\left(\ltfVar(Y_{i-1}) \: | \: \asaVar_{i-1}\right) -\E{}_{x}\sum_{i=1}^{n}\indicator(\gamma \geq i)\hffVar(Y_{i-1}) \\ = \E{}_{x}\cefVar_{n} - \E{}_{x}\sum_{i=1}^{n}\indicator(\gamma \geq i)\hffVar(Y_{i-1}),
		\end{gather*}
		where we used that $\ltfVar(x) - \hffVar(x) \geq 0$ and, for the last inequality, we also used $\indicator(\gamma \geq i) \leq \indicator(\gamma \geq i - 1)$ and replaced $n$ by $n+1$. From this we obtain
		\begin{gather}
			\E{}_{x}\sum_{i=0}^{n}\hffVar(Y_{i})\indicator(\gamma \geq i) \leq \E{}_{x}\ltfVar(X_{0}) = \ltfVar(x).
			\label{cumulative_estimation_by_V}
		\end{gather}
		Assume $\ltfVar(x) > N$. Then $\ltfVar(Y_{i}) > N$ for $i < \gamma$, by the definition of $\gamma$, and so
		\begin{gather}
			\hffVar(Y_{i}) \geq d^{-1}\gffVar(Y_{i}) > 0 \text{ for } i < \gamma
			\label{h_g_connection}
		\end{gather}
		by the definition of d. Also,
		\begin{gather}
			\hffVar(Y_{i}) \geq -H, \text{ for all } i.
			\label{h_estimation}
		\end{gather}
		by the definition of H. Using (\ref{h_g_connection}) and (\ref{h_g_connection}) in (\ref{cumulative_estimation_by_V}) we obtain:
		\begin{gather*}
			\ltfVar(x) \geq \E{}_{x}\sum_{i=0}^{n}\hffVar(Y_{i})\indicator(\gamma > i) + \E{}_{x}\sum_{i=0}^{n}\hffVar(Y_{i})\indicator(\gamma = i) \\ \geq d^{-1}\E{}_{x}\sum_{i=0}^{(\gamma - 1) \wedge n}\gffVar(Y_{i}).
		\end{gather*}
		Recall that $\gffVar(Y_{0}) + ... + \gffVar(Y_{k}) = t_{k+1}$, and so the above gives:
		\begin{gather*}
			\ltfVar(x) \geq d^{-1}\E{}_{x}t_{\gamma \wedge n}.
		\end{gather*}
		Now take limits as $n \xrightarrow{} \infty$ (both relevant sequences are increasing in n) and obtain that
		\begin{gather*}
			\E{}_{x}t_{\gamma \wedge n} \leq d\ltfVar(x).
		\end{gather*}
		It remains to see what happens if $\ltfVar(x) \leq N$. By conditioning on $Y_{1}$, we have
		\begin{gather*}
			\E{}_{x}\astVar \leq \gtfVar(x) + \E{}_{x}\left(\E{}_{Y_{1}}\astVar\indicator(\ltfVar(Y_{1}) > N)\right) \\ \leq \gffVar(x) + \E{}_{x}\left(d^{-1}\ltfVar(Y_{1})\indicator(\ltfVar(Y_{1}) > N)\right) \\ \leq \gffVar(x) + dH + d\ltfVar(x).
		\end{gather*}
		Hence,
		\begin{gather*}
			\sup_{\ltfVar(x) \leq N}\E{}_{x}\astVar \leq \sup_{\ltfVar(x) \leq N}\gffVar(x) + d(H +N),
		\end{gather*}
		where the latter is a finite constant, by assumption (L3).
	\end{proof}
	
	
	\section{Multistep Kalashikov's theorem}
	Let $\{X_{n}\}_{n \geq 0}$ be the time-homogeneous Markov chain on a state space $\assVar$. Let us define an operator $\atoVar$, which operates on the function $\ltfVar(x)$, $x \in \assVar$, in terms of the equality
	\begin{gather}
		\atoVar\ltfVar(x) = \E{}_{x}\ltfVar(X_{1}) - \ltfVar(x).
		\label{A_operator_definition}
	\end{gather}
	If the right-hand side of the equation (\ref{A_operator_definition}) is finite at point $x$, then we say that the function $\ltfVar$ belongs to the domain of definition of the operator $\atoVar$ at point $x$, and we shall designate this fact as $\ltfVar \in \akdVar(x)$. If $\ltfVar \in \akdVar(x)$ for all $x \in \assVar$, then we will write that $\ltfVar \in \akdVar$.
	
	Let $\aasVar \in \asaVar$, $\astVar_{\aasVar} = \min\{n : n > 0, X_{n} \in \aasVar\}$, and $\gtfVar(n) \geq 0$, $n \geq 0$, be a monotonic function such as $\lim_{n \xrightarrow{} \infty}\gtfVar(n) = \infty$.
	\begin{definition}
		A subset $\aasVar$ is called recurrent if for any $x \in \assVar$
		\begin{gather*}
			\Pb(\astVar_{\aasVar} < \infty \: | \: X_{0} = x) = 1.
		\end{gather*}
	\end{definition}
	\begin{definition}
		A subset $\aasVar \in \asaVar$ is called $\gtfVar$-recurrent if for any $x \in \assVar$
		\begin{gather}
			\E\{\gtfVar(\astVar_{\aasVar}) \: | \: X_{0} = x\} < \infty.
		\end{gather}
	\end{definition}
	If $\gtfVar(x) \equiv x$ then the subset $\aasVar$ is said to be (positive) recurrent. Recall the notation
	\begin{gather}
		\E{}_{x}\{\cdot\} := \E\{\cdot \: | \: X_{0} = x\}.
	\end{gather}
	\begin{repeated_theorem}
		Let us suppose that for a given function $\gffVar:\assVar \xrightarrow{} \integers$ there exists a function $\wtfVar(x, n)$, $x \in \assVar$, $n \geq 0$, which satisfies the following conditions:
		\begin{enumerate}
			\item[(a)] $\underset{x}{\inf}\wtfVar(x, n) \geq \gtfVar(n)$;
			\item[(b)] $\atoVar_{\gffVar}\wtfVar(x, n) := \E{}_{x}\wtfVar(X_{\gffVar(x)}, n+1) - \wtfVar(x, n) \leq 0$, $x \notin \aasVar$, $n \geq 0$;
			\item[(c)] $\atoVar_{\gffVar}\wtfVar(x, 0) < \infty$, $x \in \aasVar$.
		\end{enumerate}
		Then the inequality $\E{}_{x}\gtfVar(\astVar_{\aasVar}) < \infty$ holds for any $x \in \assVar$. Also the following estimation is valid:
		\begin{gather}
			\E{}_{x}\gtfVar(\astVar_{\aasVar}) \leq \begin{cases}
				\wtfVar(x, 0), & x \notin \aasVar \\
				\wtfVar(x, 0) + \atoVar_{\gffVar}\wtfVar(x, 0), & x \in \aasVar
			\end{cases}
			\label{main_G_W_inequality}
		\end{gather}
	\end{repeated_theorem}
	\begin{proof}
		We define an increasing sequence $t_{n}$ of stopping times recursively by
		\begin{gather*}
			t_{0} = 0, \: t_{n} = t_{n-1} + \gffVar(X_{t_{n-1}}), \: n \geq 1.
		\end{gather*}
		By the strong Markov property, the variables
		\begin{gather*}
			Y_{n} = X_{t_{n}}
		\end{gather*}
		form a Markov chain. One can notice, that $\{Y_{n}\}_{n \geq 0}$ form time-homogeneous Markov chain. Indeed, for any $D \in \asaVar$ and $y \in \assVar$, we have
		\begin{gather*}
			\Pb\left(Y_{n} \in D \: | \: Y_{n-1} = y\right) = \Pb\left(X_{t_{n-1} + \gffVar(X_{t_{n-1}})} \in D \: | \: X_{t_{n-1}} = y\right) \\ = \Pb\left(X_{t_{n-1} + \gffVar(y)} \in D \: | \: X_{t_{n-1}} = y\right) = \Pb{}^{\gffVar(y)}\left(D, y\right).
		\end{gather*}
		Define the stopping time
		\begin{gather*}
			\gamma = \inf\{n \geq 1 \: : \: Y_{n} \in \aasVar\} < \infty.
		\end{gather*}
		Observe that
		\begin{gather*}
			\astVar_{\aasVar} \leq t_{\gamma}, a.s.
		\end{gather*}
		Consider a new Markov chain $\{Z_{n}\}_{n \geq 0}$, where
		\begin{gather*}
			Z_{n} = (Y_{n}, n), n \geq 0.
		\end{gather*}
		Thus, the state space of this "expanded"$ $ chain is $\assVar \times \integers_{+}$ and its transition function is
		\begin{gather*}
			\Pb(Z_{n+1} = (Y_{n+1}, n+1), Y_{n+1} \in B \: | \: Y_{n}, n) = \Pb(Y_{n+1} \in B \: | \: Y_{n}),
		\end{gather*}
		so the second component of $Z_{n}$ each time has an increment 1. Note that the r.v. $\gamma$ is a stopping time not only for $\{Y_{n}\}$ but also for $\{Z_{n}\}$ either. The operator $\atoVar$ for $\{Z_{n}\}$ operates on functions defined on $\assVar \times \integers_{+}$ and the result of this is as follows:
		\begin{gather*}
			\atoVar\wtfVar(x, n) = \E{}_{x}\wtfVar(X_{1}, n+1) - \wtfVar(x, n).
		\end{gather*}
		Let $\wtfVar$ satisfy the conditions (a) - (c) of the Theorem and take the stopping times $\gamma_{n} = \min(n, \gamma)$ and $\astVar_{n} = \min(n, \astVar_{\aasVar})$. Then, by Theorem 1.1 (chapter 5, \cite{Kalashnikov}), we have
		\begin{gather}
			\E{}_{x}\gtfVar(\astVar_{n}) \leq \E{}_{x}\gtfVar(\gamma_{n}) \leq \text{(by (a))} \leq \E{}_{(x, 0)}\wtfVar(Y_{\gamma_{n}}, \gamma_{n}) \notag \notag\\ = \wtfVar(x, 0) + \E{}_{(x, 0)}\sum_{k < \gamma_{n}}\atoVar\wtfVar(Y_{k}, k) \notag\\ = \wtfVar(x, 0) + \E{}_{(x, 0)}\sum_{k < \gamma_{n}}\left[\E{}_{(Y_{k}, k)}\wtfVar(Y_{k+1}, k+1) - \wtfVar(Y_{k}, k)\right] \notag\\ = \wtfVar(x, 0) + \E{}_{(x, 0)}\sum_{k < \gamma_{n}}\left[\E{}_{(X_{t_{k}}, k)}\wtfVar(X_{t_{k} + \gffVar(X_{t_{k}})}, k+1) - \wtfVar(X_{t_{k}}, k)\right]
			\label{expectation_Gtau_estimation}
		\end{gather}
		If $x \notin \aasVar$, then $Y_{k} \notin \aasVar$ for all $k < \astVar_{n}$, and the condition (b), given that the Markov chain $X_{t_{k}}$ is time-homogeneous, implies, that the second summand on the right-hand side of (\ref{expectation_Gtau_estimation}) is nonpositive. Thus,
		\begin{gather*}
			\E{}_{x}\gtfVar(\astVar_{n}) \leq \E{}_{x}\gtfVar(\gamma_{n}) \leq \wtfVar(x, 0).
		\end{gather*}
		Letting $n \xrightarrow{} \infty$ we obtain the first line in (\ref{main_G_W_inequality}).
		
		If $x \in \aasVar$, then only the term $\atoVar\wtfVar(Y_{0}, 0) = \atoVar\wtfVar(x, 0)$ (which is finite by (c)) on the right-hand side of (\ref{expectation_Gtau_estimation}) may be positive. It follows (letting again $n \xrightarrow{} \infty$) the second line in (\ref{main_G_W_inequality}).
	\end{proof}
	
	\begin{thebibliography}{1}
		\bigskip
		\footnotesize
		\bibitem{Kalashnikov}
		Vladimir V. Kalashnikov (1994). Mathematical methods in queuing theory. - Dordrecht etc. : Kluwer acad. publ., - IX, 377 с. : ил.; 24 см. - (Mathematics a. its applications; Vol. 003271).; ISBN 0-7923-2568-0.
		
	\end{thebibliography}
\end{document}



